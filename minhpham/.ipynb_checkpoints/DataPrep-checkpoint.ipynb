{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Multiprocessing threads: 31\n",
      "reading raw data file ...\n",
      "creating shared memory objects ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/260087 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiprocessing ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260087/260087 [00:09<00:00, 26779.48it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################\n",
    "# Script to prepare data objects for training and testing\n",
    "#    1. UserFeatures: Including click history and user_portrait\n",
    "#         Click history: 1-hot encoding with 390 variables\n",
    "#         user portraits: normalize to mean 0 sd 1\n",
    "##################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import functools, pickle\n",
    "from tqdm import tqdm\n",
    "N_ITEMS = 380\n",
    "N_USER_PORTRAITS = 10\n",
    "N_THREADS = mp.cpu_count() - 1\n",
    "print('Number of Multiprocessing threads: ' + str(N_THREADS))\n",
    "###################################################################\n",
    "\n",
    "def prepareUserFeaturesTrainSet():\n",
    "    \"\"\"\n",
    "    return: nothing, save to UserFeaturesTrainSet.pkl\n",
    "        data frame with N_ITEMS+N_USER_PORTRAITS columns\n",
    "        first N_ITEMS cols: one hot encoding of clicked items\n",
    "        last N_USER_PORTRAITS cols: normalized user portraits\n",
    "    Data source: /tf/shared/trainset.csv\n",
    "    \"\"\"\n",
    "    # read data to pd dataframe\n",
    "    print('reading raw data file ...')\n",
    "    rawTrainSet = pd.read_csv('../shared/trainset.csv',' ')\n",
    "    # create output frame\n",
    "    colNames = ['clickedItem'+str(i+1) for i in range(N_ITEMS)] + ['userPortrait'+str(i+1) for i in range(N_USER_PORTRAITS)]\n",
    "    output = pd.DataFrame(data = np.zeros(shape = (rawTrainSet.shape[0], N_ITEMS+N_USER_PORTRAITS)), columns = colNames)\n",
    "    # parse each line in parallel\n",
    "    # first objects in shared memory for input and output\n",
    "    print('creating shared memory objects ... ')\n",
    "    mpManager = mp.Manager()\n",
    "    inputSharedList = mpManager.list(rawTrainSet.values.tolist())  # for memory efficiency\n",
    "    outputSharedList = mpManager.list(output.values.tolist())  # shared output as a list (because DataFrame can't)\n",
    "    p = mp.Pool(N_THREADS)\n",
    "    print('multiprocessing ... ')\n",
    "    for i in tqdm(range(rawTrainSet.shape[0])):\n",
    "        p.apply_async(parseUserFeaturesOneLine, [i, inputSharedList, outputSharedList])\n",
    "    p.close()\n",
    "    p.join()\n",
    "    # convert outputSharedList back to DataFrame\n",
    "    print('convert to DataFrame ...')\n",
    "    outputList = [x for x in outputSharedList]\n",
    "    output = pd.DataFrame(data = outputList, columns = colNames)\n",
    "#     for i in range(rawTrainSet.shape[0]):\n",
    "#         parseUserFeaturesOneLine(i, rawTrainSet, out)\n",
    "    # save to pickle file\n",
    "    return out\n",
    "#     output.to_pickle('./data/UserFeaturesTrainSet.pkl')\n",
    "\n",
    "def parseUserFeaturesOneLine(rowIndex, inputSharedList, outputSharedList):\n",
    "    \"\"\"\n",
    "    Kernel function\n",
    "    Return: Nothing, only modify outputSharedList[rowIndex]. DataFrames are passed by reference\n",
    "    Input:\n",
    "        inputSharedList: trainset or testset DataFrames converted to list\n",
    "        outputSharedList: Output as mp.Manager.list, each element of list is a list that expects N_ITEMS+N_USER_PORTRAITS element\n",
    "    ASSUMPTIONS:\n",
    "        user_click_history is on column index  1 of inputSharedList\n",
    "        user_portrait is on column index 2 of inputSharedList\n",
    "    \"\"\"\n",
    "    CLICKHIST_INDEX = 1\n",
    "    PORTRAIT_INDEX = 2\n",
    "    # parse click history, assuming \n",
    "    clickSeries = inputSharedList[rowIndex][CLICKHIST_INDEX].split(',')\n",
    "    clickedItems = [item.partition(':')[0] for item in clickSeries]\n",
    "    # add clicked items to output\n",
    "    for itemID in clickedItems:\n",
    "        if int(itemID)<=0 or int(itemID)>=N_ITEMS:  # ignore if itemID invalid\n",
    "            continue\n",
    "        colIndex = int(itemID) - 1  # index of clicked item on an element of outputSharedList\n",
    "        outputSharedList[rowIndex][colIndex] = 1\n",
    "    # parse user portraits\n",
    "    portraits = inputSharedList[rowIndex][PORTRAIT_INDEX].split(',')\n",
    "    if len(portraits)!=N_USER_PORTRAITS:\n",
    "        raise Exception(\"row \"+rowIndex+\" of data set does not have the expected number of portrait features\")\n",
    "    # add portrait features to output\n",
    "    for i in range(N_USER_PORTRAITS):\n",
    "        colIndex = N_ITEMS + i  # index of feature on an element of outputSharedList\n",
    "        outputSharedList[rowIndex][colIndex] = int(portraits[i])\n",
    "\n",
    "def preparePurchasedItemsTrainSet():\n",
    "    \"\"\"\n",
    "    return: nothing, write to PurchasedItemsTrainSet.pkl\n",
    "    Data source: /tf/shared/trainset.csv\n",
    "    \"\"\"\n",
    "    # read data to pd dataframe\n",
    "    rawTrainSet = pd.read_csv('/tf/shared/trainset.csv',' ')\n",
    "    output = []\n",
    "    for i in range(rawTrainSet.shape[0]):\n",
    "        # parse each line\n",
    "        exposedItems = rawTrainSet.exposed_items[i]\n",
    "        labels = rawTrainSet.labels[i]\n",
    "        exposedItems = exposedItems.split(',')\n",
    "        labels = labels.split(',')\n",
    "        purchasedItems = []\n",
    "        for j in range(len(labels)):\n",
    "            if int(labels[j])==1:\n",
    "                # item is purchased, append it to the purchasedItems list\n",
    "                purchasedItems.append(int(exposedItems[j]))\n",
    "        # append the list of this row to output\n",
    "        output.append(purchasedItems)\n",
    "    # write to pkl file\n",
    "    with open('/tf/shared/data/PurchasedItemsTrainSet.pkl', 'wb') as f:\n",
    "        pickle.dump(output, f)\n",
    "\n",
    "def prepareUserFeaturesTestSet():\n",
    "    \"\"\"\n",
    "    return: nothing, write to PurchasedItemsTestSet.pkl\n",
    "    Data source: /tf/shared/track1_testset.csv\n",
    "    \"\"\"\n",
    "    # read data to pd dataframe\n",
    "    print('reading raw data file ...')\n",
    "    rawTestSet = pd.read_csv('/tf/shared/track1_testset.csv',' ')\n",
    "    # create output frame\n",
    "    colNames = ['clickedItem'+str(i+1) for i in range(N_ITEMS)] + ['userPortrait'+str(i+1) for i in range(N_USER_PORTRAITS)]\n",
    "    output = pd.DataFrame(data = np.zeros(shape = (rawTestSet.shape[0], N_ITEMS+N_USER_PORTRAITS)), columns = colNames)\n",
    "    # parse each line in parallel\n",
    "    # first objects in shared memory for input and output\n",
    "    print('creating shared memory objects ... ')\n",
    "    mpManager = mp.Manager()\n",
    "    inputSharedList = mpManager.list(rawTestSet.values.tolist())  # for memory efficiency\n",
    "    outputSharedList = mpManager.list(output.values.tolist())  # shared output as a list (because DataFrame can't)\n",
    "    p = mp.Pool(N_THREADS)\n",
    "    print('multiprocessing ... ')\n",
    "    for i in tqdm(range(rawTestSet.shape[0])):\n",
    "        p.apply_async(parseUserFeaturesOneLine, [i, inputSharedList, outputSharedList])\n",
    "    p.close()\n",
    "    p.join()\n",
    "    # convert outputSharedList back to DataFrame\n",
    "    print('convert to DataFrame ...')\n",
    "    output = pd.DataFrame(data = outputSharedList, columns = colNames)\n",
    "    # write to pkl file\n",
    "    output.to_pickle('/tf/shared/data/UserFeaturesTestSet.pkl')\n",
    "        \n",
    "test = prepareUserFeaturesTrainSet()\n",
    "print(test)\n",
    "# preparePurchasedItemsTrainSet()\n",
    "\n",
    "def getUserFeaturesTrainSet():\n",
    "    \"\"\"\n",
    "    return: DataFrame with N_ITEMS+N_USER_PORTRAITS columns\n",
    "        first N_ITEMS cols: one hot encoding of clicked items\n",
    "        last N_USER_PORTRAITS cols: normalized user portraits\n",
    "    \"\"\"\n",
    "    return pd.read_pickle('./data/UserFeaturesTrainSet.pkl')\n",
    "\n",
    "def getPurchasedItemsTrainSet():\n",
    "    \"\"\"\n",
    "    return: a list, each element is a list of purchased item by a user\n",
    "    list length is same as PurchasedItemsTrainSet's nrow\n",
    "    \"\"\"\n",
    "    file = open('/tf/minhpham/data/PurchasedItemsTrainSet.pkl', 'rb')\n",
    "    data = pickle.load(file)\n",
    "    file.close()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "UserFeaturesTrainSet = getUserFeaturesTrainSet()\n",
    "for i in range(N_USER_PORTRAITS):\n",
    "    colName = 'userPortrait' + str(i+1)\n",
    "    scaler = MinMaxScaler()\n",
    "    UserFeaturesTrainSet[colName] = scaler.fit_transform(UserFeaturesTrainSet[colName].values.reshape(-1,1))\n",
    "UserFeaturesTrainSet.to_pickle('./data/UserFeaturesTrainSet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 20, 28, 99, 86, 119, 213, 237, 164], [1, 4, 26, 112, 86, 117, 191, 234], [22, 4, 28, 105], [5, 16, 1, 74, 133, 122, 235, 218], [6, 1, 16, 85, 73, 112, 239, 172, 205], [3, 19, 5, 99, 83, 92, 164, 172, 183], [8, 20, 1, 72, 100, 107, 233, 188, 164], [], [], [15, 32, 34]]\n"
     ]
    }
   ],
   "source": [
    "test = getPurchasedItemsTrainSet()\n",
    "print(test[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
